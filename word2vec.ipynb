{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGRnjqz0Onil"
   },
   "source": [
    "# Word2vec implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic8GEMZO4Fo-"
   },
   "source": [
    "read Data.xlsx and extract abstract belonging to medical category to train word embedding and clean the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1vGKerlYeOv",
    "outputId": "19ceb081-4c22-4d70-f73d-741eb92aed35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "df = pd.read_excel('Data.xlsx')\n",
    "Medical = df[df['Domain'] == 'Medical ']\n",
    "Medical_subset = Medical.head(7000)\n",
    "x_Medical = Medical_subset['Abstract'].tolist()\n",
    "# clean the abstract reading from train.txt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "x_clean = [re.sub(r'[^a-zA-Z-\\s]', u'',sentence.replace('\\n', ''), flags=re.UNICODE) for sentence in x_Medical]\n",
    "a = [list(filter(lambda w: w not in stop_words and len(w)>2, map(lemmatizer.lemmatize, sentence.lower().split()))) for sentence in x_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdWePAp04Zxp"
   },
   "source": [
    "remove tokens with less than 5 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2g_Oz2XPfEj4",
    "outputId": "c18183ae-0528-4914-db89-08748cf9ccaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14435\n"
     ]
    }
   ],
   "source": [
    "token_counts = {}\n",
    "for tokens in a:\n",
    "    for token in tokens:\n",
    "        token_counts[token] = token_counts.get(token, 0) + 1\n",
    "token_counts = {token: count for token, count in token_counts.items() if count >= 5}\n",
    "print(len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2oJw1pHf3gE",
    "outputId": "3e50e11e-c1de-4709-b3b1-6c57cad5357c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens before subsampling  917944\n"
     ]
    }
   ],
   "source": [
    "# encode each token with its index\n",
    "token_array = [token for token, _ in token_counts.items()]\n",
    "t_to_i = {token: index for index, token in enumerate(token_array)}\n",
    "data = [[t_to_i[token] for token in sentence if token in t_to_i]\n",
    "           for sentence in a]\n",
    "# calculate total number of tokens\n",
    "token_num = sum([len(sentence) for sentence in data])\n",
    "print(\"tokens before subsampling \", token_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-26RwB-lqa8"
   },
   "source": [
    "Some high-frequency words usually appear in text data, such as \"the\" \"a\" and \"in\" in English. Typically, in a background window, a word (such as \"cell\") and a lower-frequency word, such as â€œthrombocytopeniaâ€, is more beneficial to embedding the training word model than a higher-frequent word (Such as \"the\").Therefore, a training word can be sampled secondly when embedded in a model. Specifically, the data is concentrated in each indexed word $w_{i}$\n",
    "There will be a certain probability that will be discarded.\n",
    "$P\\left(w_{i}\\right)=\\max \\left(1-\\sqrt{\\frac{t}{f\\left(w_{i}\\right)}}, 0\\right)$\n",
    "f($w_{i}$)is the proportion between Data concentration word $w_{i}$ and the constant t.\n",
    "t is a superparameter (set to $10^{âˆ’4}$in the experiment)\n",
    "See only if $f (w_{i}) > t$, we can throw out the word w, and the higher the frequency of the word, the more likely it is to be discarded.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRIA_hmBtNtu",
    "outputId": "92483e52-65e2-4bdf-c6b8-0897034c6b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens after subsampling  525554\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "subsample = []\n",
    "for sentence in data:\n",
    "  dataset = []\n",
    "  for token in sentence:\n",
    "    if (0.0001 / token_counts[token_array[token]] * token_num)** 0.5 > 1  - random.uniform(0, 1):\n",
    "      dataset.append(token)\n",
    "  subsample.append(dataset)\n",
    "print(\"tokens after subsampling \",sum([len(sentence) for sentence in subsample]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L50vOdhStYmD"
   },
   "source": [
    "After sampling, there is a decrease in the frequency of words with more occurance, the number of words with less occurance remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6bavcvfwkNN",
    "outputId": "7b147e24-f362-4435-87a7-ee8880d2b346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell: before=3147, after=551\n",
      "thrombocytopenia: before=23, after=23\n"
     ]
    }
   ],
   "source": [
    "def compare(t):\n",
    "  sum_before = 0\n",
    "  for string in data:\n",
    "    sum_before += string.count(t_to_i[t])\n",
    "  sum_after = 0\n",
    "  for string in subsample:\n",
    "    sum_after += string.count(t_to_i[t])\n",
    "  print('%s: before=%d, after=%d' % (t, sum_before, sum_after))\n",
    "compare('cell')\n",
    "compare('thrombocytopenia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82V-G24hwpZr"
   },
   "source": [
    "To increase the training accuracy and efficiency, a window size is set to limit the distance from the central word to the context words. The central word is the word to be predicted. Context words are closer to central words, so there are semantic similarities between them. We randomly sample an integer between 1 and the set maximum context window size as context window size. Context words that exceed the window size will be discarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAD--SJo2pTc"
   },
   "outputs": [],
   "source": [
    "def get(dataset, window_capacity):\n",
    "  targets = []\n",
    "  contexts = []\n",
    "  for sentence in dataset:\n",
    "    if len(sentence) >=2:\n",
    "        for index in range(len(sentence)):\n",
    "          window_span = random.randrange(1, window_capacity+1)\n",
    "          targets.append(sentence[index])\n",
    "          contexts.append(sentence[max(0, index-window_span):index]+sentence[index+1:min(index+window_span+1, len(sentence))])\n",
    "  return targets, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o3bXEY_63qB"
   },
   "source": [
    "For each pair of center words and context words, we employ negative sampling to randomly select K negative samples. The sampling probability P(w) for noise words is set to the 0.75 power of the frequency of word w. This setting implies that higher frequent words are more likely to be selected during sampling, but their selection probabilities do not grow linearly with their frequencies; instead, they increase with a power of 0.75. This specific setting aims to balance the sampling probabilities between high-frequency and low-frequency words, ensuring a reasonable sampling of noise words in the negative sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DXSnNOGwZ_k"
   },
   "outputs": [],
   "source": [
    "def negs_get(contexts, weight, num):\n",
    "    neg_array = []\n",
    "    neg_samples = []\n",
    "    index = 0\n",
    "    for context in contexts:\n",
    "        negs = []\n",
    "        while len(negs) < len(context) * num:\n",
    "            if index == len(neg_samples):\n",
    "                neg_samples = random.choices(list(range(len(weight))), weight, k=int(1e5))\n",
    "                index = 0\n",
    "            if neg_samples[index] not in set(context):\n",
    "                neg = neg_samples[index]\n",
    "                negs.append(neg)\n",
    "            index = index + 1\n",
    "        neg_array.append(negs)\n",
    "    return neg_array\n",
    "\n",
    "weights = [token_counts[weight]**0.75 for weight in token_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "rpSG3cDhVvy6",
    "outputId": "6d891e60-c216-4c7d-b082-9e3993f256d8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"dataset = Data.DataLoader(Dataset(centers, contexts, negatives_array), 512, shuffle=True,\\n                            collate_fn=batches,\\n                            num_workers=4)\\nfor batch in dataset:\\n    for n, data in zip(['centers', 'contexts_negatives', 'context_negative_masks',\\n                           'context_masks'], batch):\\n        print(n, 'of shape:', data.shape)\\n    break\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, targets, backgrounds, negative_samples):\n",
    "        self.targets = targets\n",
    "        self.backgrounds = backgrounds\n",
    "        self.negs = negative_samples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.targets[i], self.backgrounds[i], self.negs[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.backgrounds)\n",
    "def batches(dataset):\n",
    "    max_length = max(len(context) + len(negative) for _, context, negative in dataset)\n",
    "    targets, context_negative_pairs, context_negative_m, context_masks = [], [], [], []\n",
    "    for t, c, n in dataset:\n",
    "        length = len(c) + len(n)\n",
    "        targets += [t]\n",
    "        context_negative_pairs += [c + n + [0] * (max_length - length)]\n",
    "        context_negative_m += [[1] * length + [0] * (max_length - length)]\n",
    "        context_masks += [[1] * len(c) + [0] * (max_length - len(c))]\n",
    "    return (torch.tensor(targets).view(-1, 1), torch.tensor(context_negative_pairs),\n",
    "            torch.tensor(context_negative_m), torch.tensor(context_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgrvF8e54pYb"
   },
   "source": [
    "\n",
    "When the mask is 1, the predicted value and label of the corresponding position will participate in the calculation of the loss function; when the mask is 0, the predicted value and label of the corresponding position will not participate in the calculation of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXdH3Orazrwk"
   },
   "outputs": [],
   "source": [
    "class SigmoidLoss(nn.Module):\n",
    "    def __init__(self): # none mean sum\n",
    "        super(SigmoidLoss, self).__init__()\n",
    "    def forward(self, into, out, m=None):\n",
    "        return nn.functional.binary_cross_entropy_with_logits(into.float(), out.float(), reduction=\"none\", weight=m.float()).mean(dim=1)\n",
    "\n",
    "sigmoid_loss = SigmoidLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoSG7gZ35h-s"
   },
   "source": [
    "The  ð‘ ð‘˜ð‘–ð‘_ð‘”ð‘Ÿð‘Žð‘š  model's input for the forward computation consists of the connected context and negative samples (contexts_and_negatives) and the central word index (center). The contexts_and_negatives variable has the shape of (batch size, max_len) and the center variable has the shape of (batch size, 1). These two variables undergo a word embedding layer conversion from word indices to word vectors before being multiplied by mini-batches to produce an output with the shape of (batch size, 1, max_len). Each component of the output is the inner product of the context word vector or negative samples vector and the center word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj_g-nhN4Y-r"
   },
   "outputs": [],
   "source": [
    "def skip_gram(target, context_negative_pairs, embed_i, embed_j):\n",
    "    return torch.bmm(embed_i(target), embed_j(context_negative_pairs).permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vabHehEe91MV"
   },
   "source": [
    "train the word embedding, training steps will be faster if GPU is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyycYPtZ3pm0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(lr, epochs, dimension, window_capability, negatives_num):\n",
    "    centers, contexts = get(subsample, window_capability)\n",
    "    negs_array = negs_get(contexts, weights, negatives_num)\n",
    "    dataset = Data.DataLoader(Dataset(centers, contexts, negs_array), 512, shuffle=True,\n",
    "                            collate_fn=batches,\n",
    "                            num_workers=4)\n",
    "    embedding = nn.Sequential(\n",
    "        nn.Embedding(num_embeddings=len(token_array), embedding_dim=dimension),\n",
    "        nn.Embedding(num_embeddings=len(token_array), embedding_dim=dimension)\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    embedding = embedding.to(device)\n",
    "    optimizer = torch.optim.Adam(embedding.parameters(), lr=lr)\n",
    "    loss_array = []\n",
    "    for i in range(epochs):\n",
    "        begin = time.time()\n",
    "        loss_total = 0.0\n",
    "        num = 0\n",
    "        for j in dataset:\n",
    "            target, context_negative_pairs, context_negative_masks, context_masks = [d.to(device) for d in j]\n",
    "\n",
    "            prediction = skip_gram(target, context_negative_pairs, embedding[0], embedding[1])\n",
    "            l = sigmoid_loss(prediction.view(context_masks.shape), context_masks, context_negative_masks)\n",
    "            loss = (l * context_negative_masks.shape[1] / context_negative_masks.float().sum(dim=1)).mean() # average loss of each batch\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_total += loss.cpu().item()\n",
    "            num += 1\n",
    "        print('epoch %d, duration %.2fs, loss %.2f'\n",
    "              % (i + 1,  time.time() - begin, loss_total / num))\n",
    "        loss_array.append(loss)\n",
    "    return embedding, loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRNn211_Kwwf"
   },
   "source": [
    "set the learning rate to 0.01, epochs to 10, embedding dimension to 100, context window size to 5, negative sample numbers to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0R15jC-8Fva",
    "outputId": "43e51996-6068-4495-f84d-9be4ae4f2cab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers of shape: torch.Size([512, 1])\n",
      "contexts_negatives of shape: torch.Size([512, 60])\n",
      "context_negative_masks of shape: torch.Size([512, 60])\n",
      "context_masks of shape: torch.Size([512, 60])\n",
      "epoch 1, loss 1.78, time 73.94s\n",
      "epoch 2, loss 0.53, time 72.73s\n",
      "epoch 3, loss 0.41, time 61.10s\n",
      "epoch 4, loss 0.37, time 65.51s\n",
      "epoch 5, loss 0.35, time 78.47s\n",
      "epoch 6, loss 0.34, time 76.10s\n",
      "epoch 7, loss 0.33, time 75.54s\n",
      "epoch 8, loss 0.32, time 67.55s\n",
      "epoch 9, loss 0.32, time 60.48s\n",
      "epoch 10, loss 0.31, time 62.08s\n"
     ]
    }
   ],
   "source": [
    "embedding, loss_array1 = train(0.01, 10, 100, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIeeNNCtHXV5"
   },
   "source": [
    "find the similar words using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPfP0gDf-BAQ",
    "outputId": "56d98bbe-d43a-4247-ff32-9aab863e6f5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.594: transcription\n",
      "cosine similarity=0.552: intracellular\n",
      "cosine similarity=0.544: expression\n",
      "cosine similarity=0.526: ap-\n",
      "cosine similarity=0.525: synthase\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(token, n, embedding):\n",
    "    weight = embedding.weight.data\n",
    "    cos_similarity = torch.matmul(weight, weight[t_to_i[token]]) / (torch.sum(weight * weight, dim=1) * torch.sum(weight[t_to_i[token]] * weight[t_to_i[token]]) + 1e-9).sqrt()\n",
    "    _, top_k_token = torch.topk(cos_similarity, k=n+1)\n",
    "    for i in top_k_token[1:]:  # delete the input word\n",
    "        print('cosine similarity=%.3f: %s' % (cos_similarity[i], (token_array[i])))\n",
    "\n",
    "get_similar_tokens('protein', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnSaF7t9Armo",
    "outputId": "ef900dec-a140-4714-95bd-b1794a412799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.731: sport\n",
      "cosine similarity=0.659: sprain\n",
      "cosine similarity=0.627: athlete\n",
      "cosine similarity=0.580: hnis\n",
      "cosine similarity=0.573: competition\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('injury', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7W176pWGpMDN",
    "outputId": "5075dc01-d472-4467-c98a-15f326b27ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.636: depressive\n",
      "cosine similarity=0.586: anxiety\n",
      "cosine similarity=0.540: psychiatric\n",
      "cosine similarity=0.531: self-esteem\n",
      "cosine similarity=0.527: membership\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('depression', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NTPcQplpuLU",
    "outputId": "dc8245c8-41a7-4531-f484-a6d99d06cab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.491: stimulation\n",
      "cosine similarity=0.480: spinal\n",
      "cosine similarity=0.477: ipsilateral\n",
      "cosine similarity=0.474: bglum\n",
      "cosine similarity=0.464: parieto-occipital\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('nerve', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WskOl_NDqETl",
    "outputId": "3a296706-1e4e-4320-a157-3a4809d8e68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.503: loss\n",
      "cosine similarity=0.477: bmi\n",
      "cosine similarity=0.461: body\n",
      "cosine similarity=0.460: normoglycemic\n",
      "cosine similarity=0.459: waist\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('weight', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHA7jPuiqJz-",
    "outputId": "9573af84-716f-4224-f940-23678a1d4ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.713: breast\n",
      "cosine similarity=0.502: radiotherapy\n",
      "cosine similarity=0.487: squamous\n",
      "cosine similarity=0.478: prostate\n",
      "cosine similarity=0.477: non-inflammatory\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('cancer', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZuySJL1qeI1",
    "outputId": "a7a042ba-a7e8-4c24-9f86-346de7ed1fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.607: vertigo\n",
      "cosine similarity=0.591: migraine\n",
      "cosine similarity=0.554: epilepsy\n",
      "cosine similarity=0.498: tth\n",
      "cosine similarity=0.493: attack\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('headache', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvbZEG-DrPDX",
    "outputId": "45a7f1d3-e9d1-4874-c7fb-a11a7d092898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.601: mellitus\n",
      "cosine similarity=0.573: hypertension\n",
      "cosine similarity=0.506: cardiovascular\n",
      "cosine similarity=0.479: lipoprotein\n",
      "cosine similarity=0.478: diabetic\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('diabetes', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ciooc1Yrl-I",
    "outputId": "d0826832-0a17-47ba-e2da-6fce9f2f9638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.584: neuron\n",
      "cosine similarity=0.564: pathology\n",
      "cosine similarity=0.543: cerebral\n",
      "cosine similarity=0.534: synaptic\n",
      "cosine similarity=0.531: nucleus\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('brain', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qryKvdv3rvGM",
    "outputId": "bac13af3-fc05-4857-af4e-daa392725e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.519: valvular\n",
      "cosine similarity=0.512: cardiac\n",
      "cosine similarity=0.510: fibrillation\n",
      "cosine similarity=0.508: arrhythmia\n",
      "cosine similarity=0.498: ecg\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('heart', 5, embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v57PHJv6r1II",
    "outputId": "30ae68d0-b1d0-4f2c-ee6a-78458068020a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity=0.553: asd\n",
      "cosine similarity=0.523: developmental\n",
      "cosine similarity=0.515: social\n",
      "cosine similarity=0.512: dsed\n",
      "cosine similarity=0.494: irritability\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('autism', 5, embedding[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
