{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq_w9Hq3pKEI"
   },
   "source": [
    "In this experiment, we adopt BERT for named entity recognition to find methodology, material, research target and metrics in each abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJU_Pq76py5y"
   },
   "source": [
    "read training dataset, train.json, extract sentences and labels which represents named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "uKcBZFSUAAyi"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = 'train.json'\n",
    "# Initialize an empty list to store the parsed JSON objects\n",
    "data_list = []\n",
    "\n",
    "# Read data from the JSON file line by line\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object and add it to the list\n",
    "        data = json.loads(line)\n",
    "        data_list.append(data)\n",
    "'''\n",
    "binary search for the first element greater than target\n",
    "'''\n",
    "def find_first_greater(array, target):\n",
    "    left, right = 0, len(array) - 1\n",
    "    result = -1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = left + (right - left) // 2\n",
    "        if array[mid] > target:\n",
    "            result = mid\n",
    "            right = mid - 1\n",
    "        else:\n",
    "            left = mid + 1\n",
    "\n",
    "    return result\n",
    "\n",
    "target_labels = []\n",
    "target_sentences = []\n",
    "# for each abstract\n",
    "for data in data_list:\n",
    "  target_ner = []\n",
    "  ner_index = []\n",
    "  sentences_length = []\n",
    "  # find the length of each sentence in abstract\n",
    "  for sentence in data['sentences']:\n",
    "    sentences_length.append(len(sentence))\n",
    "\n",
    "  labels = []\n",
    "  beginning_index = [0]\n",
    "  for i in range(1,len(sentences_length)):\n",
    "    beginning_index.append(beginning_index[i-1]+sentences_length[i-1])\n",
    "    labels.append(['o']*sentences_length[i-1])\n",
    "  labels.append(['o']*sentences_length[len(sentences_length)-1])\n",
    "\n",
    "  # construct labels\n",
    "  j = 0\n",
    "  for ner_group in data['ner']:\n",
    "      found = False\n",
    "      for ner_item in ner_group:\n",
    "          #if 'method' in ner_item[2].lower() or 'task' in ner_item[2].lower() or 'material' in ner_item[2].lower() or 'generic' in ner_item[2].lower() or 'metric' in ner_item[2].lower() or 'otherscientificterm' in ner_item[2].lower():\n",
    "          if 'method' in ner_item[2].lower() or 'task' in ner_item[2].lower() or 'material' in ner_item[2].lower() or 'generic' in ner_item[2].lower() or 'metric' in ner_item[2].lower():\n",
    "              index = find_first_greater(beginning_index, ner_item[0])\n",
    "              if index == -1:\n",
    "                index = len(labels)\n",
    "              if 'method' in ner_item[2].lower():\n",
    "                for i in range(ner_item[0] - beginning_index[index-1],ner_item[1] - beginning_index[index-1]+1):\n",
    "                  labels[index-1][i] = 'me'\n",
    "              elif 'task' in ner_item[2].lower():\n",
    "                for i in range(ner_item[0] - beginning_index[index-1],ner_item[1] - beginning_index[index-1]+1):\n",
    "                  labels[index-1][i] = 't'\n",
    "              elif 'material' in ner_item[2].lower():\n",
    "                for i in range(ner_item[0] - beginning_index[index-1],ner_item[1] - beginning_index[index-1]+1):\n",
    "                  labels[index-1][i] = 'ma'\n",
    "              elif 'generic' in ner_item[2].lower():\n",
    "                for i in range(ner_item[0] - beginning_index[index-1],ner_item[1] - beginning_index[index-1]+1):\n",
    "                  labels[index-1][i] = 'ge'\n",
    "              elif 'metric' in ner_item[2].lower():\n",
    "                for i in range(ner_item[0] - beginning_index[index-1],ner_item[1] - beginning_index[index-1]+1):\n",
    "                  labels[index-1][i] = 'met'\n",
    "              '''elif 'otherscientificterm' in ner_item[2].lower():\n",
    "                for i in range(ner_item[0] - beginning_index[index-1],ner_item[1] - beginning_index[index-1]+1):\n",
    "                  labels[index-1][i] = 'ot'\n",
    "              '''\n",
    "              found = True\n",
    "      if found == True:\n",
    "          target_ner.append(ner_group)\n",
    "          ner_index.append(j)\n",
    "      j+=1\n",
    "\n",
    "  i = 0\n",
    "  for sentence in data['sentences']:\n",
    "    if i in ner_index:\n",
    "      target_sentences.append(sentence)\n",
    "      target_labels.append(labels[i])\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHYtUM7uqNsA"
   },
   "source": [
    "write the sentence and labels into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqJ4EEj4AKrp",
    "outputId": "456a75e9-5214-444c-942a-ed4dca550cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "target_sentences = [' '.join(sentence) for sentence in target_sentences]\n",
    "target_labels = [' '.join(label) for label in target_labels]\n",
    "# Create a DataFrame from the arrays\n",
    "df = pd.DataFrame({'text': target_sentences, 'labels': target_labels})\n",
    "\n",
    "# Specify the CSV file name\n",
    "csv_file = 'output.csv'\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.to_csv(csv_file, index=True)\n",
    "\n",
    "print(\"Data has been written to\", csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "1657ccc8-b9dd-46e7-a08f-b9176ea274ba"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "438f352b-1664-4219-b257-855919d467fa"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e5536782-4d9a-4c35-9d22-7da36a08911a",
    "outputId": "19bc0156-fd1f-4ee6-e171-d60e88ef8b8b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8434ea29-3baf-44e4-8005-ed45b95c5486\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>English is shown to be trans-context-free on t...</td>\n",
       "      <td>ma o o o o o o o o o o o o o o o o o o o o o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The agreement in question involves number in n...</td>\n",
       "      <td>o ge o o o o o o o o o o o o o o o o o o o o o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The formal proof , which makes crucial use of ...</td>\n",
       "      <td>o o o o o o o o o o me me o o o o o o o o o o ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In this paper , a novel method to learn the in...</td>\n",
       "      <td>o o o o o o me o o o o o o o t t t o o o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Based on this assumption , firstly we derived ...</td>\n",
       "      <td>o o o o o o o o o me me me me me me o t t t me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8434ea29-3baf-44e4-8005-ed45b95c5486')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8434ea29-3baf-44e4-8005-ed45b95c5486 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8434ea29-3baf-44e4-8005-ed45b95c5486');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-aea5c99d-48da-49d8-b77f-a9a51ba32426\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aea5c99d-48da-49d8-b77f-a9a51ba32426')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-aea5c99d-48da-49d8-b77f-a9a51ba32426 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           0  English is shown to be trans-context-free on t...   \n",
       "1           1  The agreement in question involves number in n...   \n",
       "2           2  The formal proof , which makes crucial use of ...   \n",
       "3           3  In this paper , a novel method to learn the in...   \n",
       "4           4  Based on this assumption , firstly we derived ...   \n",
       "\n",
       "                                              labels  \n",
       "0       ma o o o o o o o o o o o o o o o o o o o o o  \n",
       "1  o ge o o o o o o o o o o o o o o o o o o o o o...  \n",
       "2  o o o o o o o o o o me me o o o o o o o o o o ...  \n",
       "3           o o o o o o me o o o o o o o t t t o o o  \n",
       "4  o o o o o o o o o me me me me me me o t t t me...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8eQCUmxq74w"
   },
   "source": [
    "initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "d41be369-ee57-4949-aeb8-7960746d2aea"
   },
   "outputs": [],
   "source": [
    "tk = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ-BfxukwqKM"
   },
   "source": [
    "use $word\\_ids$ to identify whether sub-words belong to the same original word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ac7f0682-ea50-4aeb-bcd3-9230df735554"
   },
   "outputs": [],
   "source": [
    "# Flag to determine whether all tokens have been identified\n",
    "tokens_all_identified = False\n",
    "# Function to align the labels with the tokenized text\n",
    "def generate_labels(text_array, label_array):\n",
    "    # Tokenize the text with a maximum length of 512 and pad as needed\n",
    "    tokenized_text = tk(text_array, padding='max_length', max_length=512, truncation=True)\n",
    "    # Get the word IDs from the tokenized text\n",
    "    text_ids = tokenized_text.word_ids()\n",
    "\n",
    "    previous = None\n",
    "    labels = []\n",
    "\n",
    "    for text_id in text_ids:\n",
    "        # If the ID is None, assign a padding token ID of -100\n",
    "        if text_id is None:\n",
    "            labels.append(-100)\n",
    "        # if the previous token and current token belongs to different words, set the label to the current word id\n",
    "        elif text_id != previous:\n",
    "            try:\n",
    "                labels.append(label_to_id[label_array[text_id]])\n",
    "            except:\n",
    "                labels.append(-100)\n",
    "        else:\n",
    "            # if the previous token and current token belongs to the same words, set the label to -100\n",
    "            try:\n",
    "                labels.append(label_to_id[label_array[text_id]] if tokens_all_identified else -100)\n",
    "            except:\n",
    "                labels.append(-100)\n",
    "        previous = text_id\n",
    "\n",
    "    return labels\n",
    "# process the dataset in batches\n",
    "class BatchData(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "\n",
    "        self.text_array = [tk(str(t), padding='max_length', truncation=True, max_length = 512, return_tensors=\"pt\") for t in dataframe['text'].values.tolist()]\n",
    "        self.label_array = [generate_labels(x,y) for x,y in zip(dataframe['text'].values.tolist(), [t.split() for t in dataframe['labels'].values.tolist()])]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.text_array[index], torch.LongTensor(self.label_array[index])\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYBzmsm-w4YN"
   },
   "source": [
    "construct training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "6599961c-1cda-47bf-8c82-a1f9ebc94a95"
   },
   "outputs": [],
   "source": [
    "df_subsample = df[0:1000]\n",
    "labels = []\n",
    "for l in df_subsample['labels']:\n",
    "    labels.append(l.split())\n",
    "\n",
    "# construct unique label set\n",
    "label_set = set()\n",
    "for label in labels:\n",
    "    for l in label:\n",
    "        if l not in label_set:\n",
    "            label_set.add(l)\n",
    "\n",
    "# map the label to id and id to label\n",
    "label_to_id = {key: value for value, key in enumerate(label_set)}\n",
    "id_to_label = {value: key for value, key in enumerate(label_set)}\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_frac = 0.8\n",
    "df_shuffled = shuffle(df_subsample, random_state=42)\n",
    "split_index = int(train_frac * len(df_shuffled))\n",
    "data_train = df_shuffled.iloc[:split_index]\n",
    "data_val = df_shuffled.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqaStzyUxNW3"
   },
   "source": [
    " note that tokens not representing named entities are more prevalent than those for named entities. To address this data imbalance issue, when calculating the cross-entropy loss, we will set the weight for tokens tokens not belonging to named entities ( 'o' ) to 0.5, and the weight for named entities will be set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "aZp6iUcACuDq"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# Assuming labels are integers starting from 0 to len(unique_labels)-1\n",
    "# Adjust this according to your labels\n",
    "class_weights = torch.ones(len(label_set))\n",
    "# For example, if you wish to make the weight of 'O' label smaller, do something like:\n",
    "O_label_index = label_to_id[\"o\"]  # Assuming label_to_index is a dictionary that maps label to its index\n",
    "class_weights[O_label_index] = 0.5  # Halving the weight for 'O' label\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClEo2fVi0aJf"
   },
   "source": [
    "define a BERT-based model for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "13ebfa5e-c91a-4967-b0cc-23e314c32348"
   },
   "outputs": [],
   "source": [
    "class Bert(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Bert, self).__init__()\n",
    "        # load the pre-trained BERT model for token classification\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_set))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        # pass the data and labels to BERT model\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmxsNYZ2fhg"
   },
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(m, data_train, data_val):\n",
    "    # load the data in batches\n",
    "    train_data_batch = BatchData(data_train)\n",
    "    val_data_batch = BatchData(data_val)\n",
    "\n",
    "    train_loader = DataLoader(train_data_batch, batch_size=5, num_workers=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_data_batch, batch_size=5, num_workers=4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # apply SGD optimizer\n",
    "    opt = SGD(m.parameters(), lr=5e-3)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        m = m.cuda()\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        acc_sum = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        m.train()\n",
    "\n",
    "        for data, label in tqdm(train_loader):\n",
    "            # feed label and training dataset to BERT model for classification\n",
    "            input_ids, attention_mask,labels = data['input_ids'].squeeze(1).to(device), data['attention_mask'].squeeze(1).to(device), label.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            _,results = m(input_ids, attention_mask, labels)\n",
    "            # calculate cross validation loss\n",
    "            loss_func = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100) # Ignoring the -100 label\n",
    "            l = loss_func(results.view(-1, len(label_set)), labels.view(-1))\n",
    "\n",
    "\n",
    "            for j in range(results.shape[0]):\n",
    "\n",
    "              results_clean = results[j][labels[j] != -100]\n",
    "\n",
    "              pred = results_clean.argmax(dim=1)\n",
    "              acc_sum += (pred == labels[j][labels[j] != -100]).float().mean()\n",
    "              loss_sum += l.item()\n",
    "\n",
    "            l.backward()\n",
    "            opt.step()\n",
    "\n",
    "        m.eval()\n",
    "        train_accuracy = acc_sum / len(data_train)\n",
    "        average_train_loss = loss_sum / len(data_train)\n",
    "        \n",
    "        val_acc_sum = 0\n",
    "        val_loss_sum = 0\n",
    "        # calculate the accuracy and loss for validation dataset\n",
    "        for data, label in val_loader:\n",
    "\n",
    "            labels = label.to(device)\n",
    "            l, results = m(data['input_ids'].squeeze(1).to(device), data['attention_mask'].squeeze(1).to(device), labels)\n",
    "\n",
    "            for j in range(results.shape[0]):\n",
    "\n",
    "              results_val = results[j][labels[j] != -100]\n",
    "              val_acc_sum += (results_val.argmax(dim=1) == labels[j][labels[j] != -100]).float().mean()\n",
    "              val_loss_sum += l.item()\n",
    "\n",
    "        val_accuracy = val_acc_sum / len(data_val)\n",
    "        average_val_loss = val_loss_sum / len(data_val)\n",
    "\n",
    "        print(\n",
    "            f'epoch: {i + 1} , loss of training dataset: {average_train_loss: .3f}, accuracy of training dataset: {train_accuracy: .3f} , loss of validation dataset: {average_val_loss: .3f}, accuracy of validation dataset: {val_accuracy: .3f}')\n",
    "\n",
    "m = Bert()\n",
    "train(m, data_train, data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKuAtYAQ71Os"
   },
   "source": [
    "extract noun chunks, verbs between noun chunks and ADP between noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "fQOTB82FAG3C"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_verbs_between_noun_chunks(sentence):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(sentence)\n",
    "    # extract the list of noun chunks\n",
    "    noun_chunks = list(doc.noun_chunks)\n",
    "    noun_chunks_except_PRON = []\n",
    "    for chunk in noun_chunks:\n",
    "        nouns = [token.i for token in chunk if token.pos_ != 'PRON']\n",
    "        if nouns:\n",
    "            # Use indices to create a Span object\n",
    "            noun_span = doc[nouns[0]: nouns[-1]+1]\n",
    "            noun_chunks_except_PRON.append(noun_span)\n",
    "    # extract verbs and ADP between noun chunks\n",
    "    verbs_between_chunks = []\n",
    "    ADP_between_chunks = []\n",
    "    for i in range(len(noun_chunks_except_PRON)-1):\n",
    "        start = noun_chunks_except_PRON[i].end\n",
    "        end = noun_chunks_except_PRON[i+1].start\n",
    "\n",
    "        verbs_between = [token.text for token in doc[start:end] if token.pos_ == 'VERB']\n",
    "        ADP_between = [token.text for token in doc[start:end] if token.pos_ == 'ADP' or token.pos_ == 'CCONJ']\n",
    "        verbs_between_chunks.append(verbs_between)\n",
    "        ADP_between_chunks.append(ADP_between)\n",
    "\n",
    "    return noun_chunks_except_PRON, verbs_between_chunks,ADP_between_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP2DRSsgBO1d"
   },
   "source": [
    "extract the PROPN chunks from Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "FdGs83QasmSi"
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "def find_containing_element(str_list, target_string):\n",
    "    for element in str_list:\n",
    "        if target_string in element:\n",
    "            return element\n",
    "    return None\n",
    "\n",
    "def find_PROPN(sentence):\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  doc = nlp(sentence)\n",
    "  tokens = word_tokenize(sentence)\n",
    "  # Initialize an empty Span list\n",
    "  noun_spans = []\n",
    "\n",
    "  # Initialize None objects for start and end tokens\n",
    "  start_token, end_token = None, None\n",
    "  # Iterate over the tokens\n",
    "  for token in doc:\n",
    "      # If this is a 'PROPN' or 'NOUN' token and we haven't seen a 'PROPN' or 'NOUN' before, mark its position\n",
    "      if (token.pos_ in ['PROPN', 'NOUN']) and start_token is None:\n",
    "          start_token = token.i\n",
    "          end_token = token.i\n",
    "      # If this is a 'PROPN' or 'NOUN' token and we have seen a 'PROPN' or 'NOUN' before, update the end position\n",
    "      elif (token.pos_ in ['PROPN', 'NOUN']) and start_token is not None:\n",
    "          end_token = token.i\n",
    "      # If this is not a 'PROPN' or 'NOUN' token and we have seen a 'PROPN' or 'NOUN' before, create a Span for the sequence\n",
    "      elif (token.pos_ not in ['PROPN', 'NOUN']) and start_token is not None:\n",
    "          # Check if any of the tokens in the sequence is a 'PROPN'\n",
    "          if any(t.pos_ == 'PROPN' for t in doc[start_token:end_token+1]):\n",
    "              noun_spans.append(Span(doc, start_token, end_token+1))\n",
    "          start_token, end_token = None, None\n",
    "  # Check for the last sequence in the Doc\n",
    "  if start_token is not None and any(t.pos_ == 'PROPN' for t in doc[start_token:end_token+1]):\n",
    "      noun_spans.append(Span(doc, start_token, end_token+1))\n",
    "\n",
    "  full_result_array = []\n",
    "  for span in noun_spans:\n",
    "      full_result = \"\"\n",
    "      for element in span.text.split():\n",
    "          if element not in tokens:\n",
    "              result = find_containing_element(tokens, element)\n",
    "              if result is not None and result not in full_result:\n",
    "                  full_result+=result+ \" \"\n",
    "          else:\n",
    "              full_result+=element+\" \"\n",
    "      full_result_array.append(full_result.strip())\n",
    "  return full_result_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZgKF2D6GK9h"
   },
   "source": [
    "predict on testing dataset, return noun chunks and PROPN chunks of corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "99bc3835-a075-4fce-812b-7b9e96778816"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "'''\n",
    "The output predictions from BERT include labels for both complete words and subwords generated during tokenization. We need to utilize the $word\\_ids$ to identify and discard the subwords and establish a one-to-one relationship between words and their corresponding labels.\n",
    "'''\n",
    "def generate_testing_labels(text_array):\n",
    "    # Tokenize the text with a maximum length of 512 and pad as needed\n",
    "    tokenized_text = tk(text_array, padding='max_length', max_length=512, truncation=True)\n",
    "    # Get the word IDs from the tokenized text\n",
    "    text_ids = tokenized_text.word_ids()\n",
    "\n",
    "    previous = None\n",
    "    labels = []\n",
    "\n",
    "    for text_id in text_ids:\n",
    "        # If the ID is None, assign a padding token ID of -100\n",
    "        if text_id is None:\n",
    "            labels.append(-100)\n",
    "        # if the previous token and current token belongs to different words, set the label to 1\n",
    "        elif text_id != previous:\n",
    "            try:\n",
    "                labels.append(1)\n",
    "            except:\n",
    "                labels.append(-100)\n",
    "        else:\n",
    "            # if the previous token and current token belongs to the same words, set the label to -100\n",
    "            try:\n",
    "                labels.append(1 if tokens_all_identified else -100)\n",
    "            except:\n",
    "                labels.append(-100)\n",
    "        previous = text_id\n",
    "\n",
    "    return labels\n",
    "\n",
    "def evaluate_one_text(m, st):\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        m = m.cuda()\n",
    "\n",
    "    text = tk(st, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    labels = torch.Tensor(generate_testing_labels(st)).unsqueeze(0).to(device)\n",
    "\n",
    "    results = m(text['input_ids'].to(device), text['attention_mask'].to(device), None)\n",
    "    results = results[0][labels != -100]\n",
    "\n",
    "    prediction = [id_to_label[i] for i in results.argmax(dim=1).tolist()]\n",
    "    print(st)\n",
    "    print(prediction)\n",
    "    tokens = []\n",
    "    texts = word_tokenize(st)\n",
    "    for text in texts:\n",
    "        tokens += [token for token in re.split(r'([^a-zA-Z0-9])', text) if token]\n",
    "    # extract the index of corresponding labels\n",
    "    ma_index = [index for index, value in enumerate(prediction) if value == 'ma']\n",
    "    t_index = [index for index, value in enumerate(prediction) if value == 't']\n",
    "    me_index = [index for index, value in enumerate(prediction) if value == 'me']\n",
    "    ge_index = [index for index, value in enumerate(prediction) if value == 'ge']\n",
    "    met_index = [index for index, value in enumerate(prediction) if value == 'met']\n",
    "    other_index = [index for index, value in enumerate(prediction) if value == 'ot']\n",
    "    # extract tokens between corresponding index\n",
    "    '''material = [tokens[i] for i in ma_index]\n",
    "    target = [tokens[i] for i in t_index]\n",
    "    method = [tokens[i] for i in me_index]\n",
    "    genric = [tokens[i] for i in ge_index]'''\n",
    "    metric = [tokens[i] for i in met_index]\n",
    "    #other = [tokens[i] for i in other_index]\n",
    "    # extract nouns, verbs and ADP from abstract sentences\n",
    "    noun_chunks_except_PRON, verbs_between_chunks,ADP_between_chunks = extract_verbs_between_noun_chunks(st)\n",
    "    # extract PROPN chunks from abstract sentences\n",
    "    PROPN = find_PROPN(st)\n",
    "    # extract the material, research target, method from abstract\n",
    "    matching_material_array = []\n",
    "    if len(ma_index) > 0:\n",
    "      for mat in ma_index:\n",
    "        matching_material = [chunk.text for chunk in noun_chunks_except_PRON if mat>=chunk.start and mat < chunk.end]\n",
    "        if matching_material:\n",
    "          if matching_material[0] not in matching_material_array:\n",
    "            matching_material_array+=matching_material\n",
    "        else:\n",
    "          for prop in PROPN:\n",
    "            if tokens[mat] in prop and prop not in matching_material_array:\n",
    "              matching_material_array+=[prop]\n",
    "\n",
    "    matching_target_array = []\n",
    "    if len(t_index) > 0:\n",
    "      for ta in t_index:\n",
    "        matching_target = [chunk.text for chunk in noun_chunks_except_PRON if ta>=chunk.start and ta < chunk.end]\n",
    "        if matching_target:\n",
    "          if matching_target[0] not in matching_target_array:\n",
    "            matching_target_array+=matching_target\n",
    "        else:\n",
    "          for prop in PROPN:\n",
    "            if tokens[ta] in prop and prop not in matching_target_array:\n",
    "              matching_target_array+=[prop]\n",
    "\n",
    "    matching_method_array = []\n",
    "    if len(me_index) > 0:\n",
    "      for me in me_index:\n",
    "        matching_method = [chunk.text for chunk in noun_chunks_except_PRON if me>=chunk.start and me < chunk.end]\n",
    "        if matching_method:\n",
    "          if matching_method[0] not in matching_method_array:\n",
    "            matching_method_array+=matching_method\n",
    "        else:\n",
    "          for prop in PROPN:\n",
    "            if tokens[me] in prop and prop not in matching_method_array:\n",
    "              matching_method_array+=[prop]\n",
    "\n",
    "    matching_genric_array = []\n",
    "    if len(ge_index) > 0:\n",
    "      for ge in ge_index:\n",
    "        matching_genric = [chunk.text for chunk in noun_chunks_except_PRON if ge>=chunk.start and ge < chunk.end]\n",
    "        if matching_genric:\n",
    "          if matching_genric[0] not in matching_genric_array:\n",
    "            matching_genric_array+=matching_genric\n",
    "        else:\n",
    "          for prop in PROPN:\n",
    "            if tokens[ge] in prop and prop not in matching_genric_array:\n",
    "              matching_genric_array+=[prop]\n",
    "\n",
    "    '''matching_metric_array = []\n",
    "    if len(metric) > 0:\n",
    "      for me in metric:\n",
    "        matching_metric = [chunk.text for chunk in noun_chunks_except_PRON if me in chunk.text]\n",
    "        if matching_metric:\n",
    "          if matching_metric[0] not in matching_metric_array:\n",
    "            matching_metric_array+=matching_metric\n",
    "        else:\n",
    "          for prop in PROPN:\n",
    "            if me in prop and prop not in matching_metric_array:\n",
    "              matching_metric_array+=[prop]'''\n",
    "    matching_metric_array = metric\n",
    "    matching_other_array = []\n",
    "    if len(other_index) > 0:\n",
    "      for ot in other_index:\n",
    "        matching_other = [chunk.text for chunk in noun_chunks_except_PRON if ot>=chunk.start and ot < chunk.end]\n",
    "        if matching_other:\n",
    "          if matching_other[0] not in matching_other_array:\n",
    "            matching_other_array+=matching_other\n",
    "        else:\n",
    "          for prop in PROPN:\n",
    "            if tokens[ot] in prop and prop not in matching_other_array:\n",
    "              matching_other_array+=[prop]\n",
    "    total_length = len(matching_material_array) + len(matching_target_array) + len(matching_method_array) + len(matching_genric_array) + len(matching_metric_array) + len(matching_other_array)\n",
    "    #total_length = len(matching_material_array) + len(matching_target_array) + len(matching_method_array) + len(matching_genric_array) + len(matching_metric_array)\n",
    "    domain_length = 0\n",
    "    if len(matching_material_array)> 0:\n",
    "      domain_length +=1\n",
    "    if len(matching_method_array) > 0:\n",
    "      domain_length += 1\n",
    "    if len(matching_genric_array) > 0:\n",
    "      domain_length += 1\n",
    "    if len(matching_metric_array) > 0:\n",
    "      domain_length += 1\n",
    "    if len(matching_target_array) > 0:\n",
    "      domain_length += 1\n",
    "    '''if len(matching_other_array) > 0:\n",
    "      domain_length += 1'''\n",
    "    '''print(matching_material_array)\n",
    "    print(matching_method_array)\n",
    "    print(matching_genric_array)\n",
    "    print(matching_metric_array)\n",
    "    print(matching_target_array)\n",
    "    print(matching_other_array)'''\n",
    "\n",
    "    return matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array,domain_length*2 + total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgVA8HOXJYD0"
   },
   "source": [
    "The testing dataset, which contains Abstract from medical, physics and astronomy categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fa186d96-7e3c-4457-a3ab-cabc61f2d261"
   },
   "outputs": [],
   "source": [
    "text2 = \"The contraction of the heart muscle is triggered by self-organizing electrical patterns. Abnormalities in these patterns lead to cardiac arrhythmias, a prominent cause of mortality worldwide. The targeted treatment or prevention of arrhythmias requires a thorough understanding of the interacting wavelets, vortices and conduction block sites within the excitation pattern. Currently, there is no conceptual framework that covers the elementary processes during arrhythmogenesis in detail, in particular the transient pivoting patterns observed in patients, which can be interleaved with periods of less fragmented waves. Here, we provide such a framework in terms of quasiparticles and Feynman diagrams, which were originally developed in theoretical physics. We identified three different quasiparticles in excitation patterns: heads, tails and pivots. In simulations and experiments, we show that these basic building blocks can combine into at least four different bound states. By representing their interactions as Feynman diagrams, the creation and annihilation of rotor pairs are shown to be sequences of dynamical creation, annihilation and recombination of the identified quasiparticles. Our results provide a new theoretical foundation for a more detailed theory, analysis and mechanistic insights of topological transitions in excitation patterns, to be applied within and beyond the context of cardiac electrophysiology.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1n5X0dJMpxH6"
   },
   "outputs": [],
   "source": [
    "text1 = \"Introduction Large healthcare datasets can provide insight that has the potential to improve outcomes for patients. However, it is important to understand the strengths and limitations of such datasets so that the insights they provide are accurate and useful. The aim of this study was to identify data inconsistencies within the Hospital Episodes Statistics (HES) dataset for autistic patients and assess potential biases introduced through these inconsistencies and their impact on patient outcomes. The study can only identify inconsistencies in recording of autism diagnosis and not whether the inclusion or exclusion of the autism diagnosis is the error. Data were extracted from the HES database for the period 1st April 2013 to 31st March 2021 for patients with a diagnosis of autism. First spells in hospital during the study period were identified for each patient and these were linked to any subsequent spell in hospital for the same patient. Data inconsistencies were recorded where autism was not recorded as a diagnosis in a subsequent spell. Features associated with data inconsistencies were identified using a random forest classifiers and regression modelling. Results Data were available for 172,324 unique patients who had been recorded as having an autism diagnosis on first admission. In total, 43.7 % of subsequent spells were found to have inconsistencies. The features most strongly associated with inconsistencies included greater age, greater deprivation, longer time since the first spell, change in provider, shorter length of stay, being female and a change in the main specialty description. The random forest algorithm had an area under the receiver operating characteristic curve of 0.864 in predicting a data inconsistency. For patients who died in hospital, inconsistencies in their final spell were significantly associated with being 80 years and over, being female, greater deprivation and use of a palliative care code in the death spell. conclusions Data inconsistencies in the HES database were relatively common in autistic patients and were associated a number of patient and hospital admission characteristics. Such inconsistencies have the potential to distort our understanding of service use in key demographic groups.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dkVWed0-iyhd"
   },
   "outputs": [],
   "source": [
    "text9 = \"Observed scaling relations in galaxies between baryons and dark matter global properties are key to shed light on the process of galaxy formation and on the nature of dark matter. Here, we study the scaling relation between the neutral hydrogen (HI) and dark matter mass in isolated rotationally-supported disk galaxies at low redshift. We first show that state-of-the-art galaxy formation simulations predict that the HI-to-dark halo mass ratio decreases with stellar mass for the most massive disk galaxies. We then infer dark matter halo masses from high-quality rotation curve data for isolated disk galaxies in the local Universe, and report on the actual universality of the HI-to-dark halo mass ratio for these observed galaxies. This scaling relation holds for disks spanning a range of 4 orders of magnitude in stellar mass and 3 orders of magnitude in surface brightness. Accounting for the diversity of rotation curve shapes in our observational fits decreases the scatter of the HI-to-dark halo mass ratio while keeping it constant. This finding extends the previously reported discrepancy for the stellar-to-halo mass relation of massive disk galaxies within galaxy formation simulations to the realm of neutral atomic gas. Our result reveals that isolated galaxies with regularly rotating extended HI disks are surprisingly self-similar up to high masses, which hints at mass-independent self-regulation mechanisms that have yet to be fully understood.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8d8Etfrniyhd"
   },
   "outputs": [],
   "source": [
    "text8 = \"The formation history of giant planets inside and outside the solar system remains unknown. We suggest that runaway gas accretion is initiated only at a mass of ~100 M_Earth and that this mass corresponds to the transition to a gas giant, a planet that its composition is dominated in hydrogen and helium. Delaying runaway accretion to later times (a few Myr) and higher masses is likely to be a result of an intermediate stage of efficient heavy-element accretion (at a rate of ~10^-5 M_Earth/yr) that provides sufficient energy to hinder rapid gas accretion. This may imply that Saturn has never reached runaway gas accretion, and that it is a failed giant planet. The transition to a gas giant planet above Saturn's mass naturally explains the differences between the bulk metallicities and internal structures of Jupiter and Saturn. The transition mass to a gas giant planets strongly depends on the exact formation history and birth environment of the planets, which are still not well constrained for our Solar System. In terms of giant exoplanets, delaying runaway gas accretion to planets beyond Saturn's mass can explain the transitions in the mass-radius relations of observed exoplanets and the high metallicity of intermediate-mass exoplanets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fIFDPmjCiyhc"
   },
   "outputs": [],
   "source": [
    "text7 = \"Entropy measures quantify the amount of information and correlations present in a quantum system. In practice, when the quantum state is unknown and only copies thereof are available, one must resort to the estimation of such entropy measures. Here we propose a variational quantum algorithm for estimating the von Neumann and Rényi entropies, as well as the measured relative entropy and measured Rényi relative entropy. Our approach first parameterizes a variational formula for the measure of interest by a quantum circuit and a classical neural network, and then optimizes the resulting objective over parameter space. Numerical simulations of our quantum algorithm are provided, using a noiseless quantum simulator. The algorithm provides accurate estimates of the various entropy measures for the examples tested, which renders it as a promising approach for usage in downstream tasks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BHspgPANiyhc"
   },
   "outputs": [],
   "source": [
    "text6 = \"A challenging aspect of the description of a tokamak disruption is evaluating the hot tail runaway electron (RE) seed that emerges during the thermal quench. This problem is made challenging due to the requirement of describing a strongly non-thermal electron distribution, together with the need to incorporate a diverse range of multiphysics processes including magnetohydrodynamic instabilities, impurity transport, and radiative losses. The present work develops a physics-informed neural network (PINN) tailored to the solution of the hot tail seed during an idealized axisymmetric thermal quench. Here, a PINN is developed to identify solutions to the adjoint relativistic Fokker-Planck equation in the presence of a rapid quench of the plasma's thermal energy. It is shown that the PINN is able to accurately predict the hot tail seed across a range of parameters including the thermal quench time scale, initial plasma temperature, and local current density, in the absence of experimental or simulation data. The hot tail PINN is verified by comparison with a direct Monte Carlo solution, with excellent agreement found across a broad range of thermal quench conditions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "tffScNqjiyhb"
   },
   "outputs": [],
   "source": [
    "text5 = \"The advancement of neutrino observatories has sparked a surge in multi-messenger astronomy. Multiple neutrino associations among blazars are reported while neutrino production sites are located within their central (sub)parsecs. Yet, many questions remain on the nature of those processes. The next generation Event Horizon Telescope (ngEHT) is uniquely positioned for these studies, as its high frequency and resolution can probe both the accretion disk region and the parsec-scale jet. This opens up new opportunities for connecting the two regions and unraveling the proton acceleration and neutrino production in blazars. We outline observational strategies for ngEHT and highlight what it can contribute to the multi-messenger study of blazars.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VPqIGEHuiyhb"
   },
   "outputs": [],
   "source": [
    "text4 = \"We show that the transformation of a time-evolving spherically symmetric metric tensor into a Painleve-Gullstrand-Lemaitre form brings forth a few curious consequences. The time evolution describes a non-singular gravitational collapse, leading to a bounce and dispersal of all the clustered matter, or a wormhole geometry for certain initial conditions. The null convergence condition is violated only at the onset of bounce or the wormhole formation. As an example, the requirements to develop a Simpson-Visser wormhole/regular black-hole geometry is discussed. The solution can be regarded as a new time-evolving twin of sonic dumb holes found in analog gravity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "03Jsf9yJEMS0"
   },
   "outputs": [],
   "source": [
    "text = \"Background To gain maximum insight from large administrative healthcare datasets it is important to understand their data quality. Although a gold standard against which to assess criterion validity rarely exists for such datasets, internal consistency can be evaluated. We aimed to identify inconsistencies in the recording of mandatory International Statistical Classification of Diseases and Related Health Problems, tenth revision (ICD- 10) codes within the Hospital Episodes Statistics dataset in England. Methods Three exemplar medical conditions where recording is mandatory once diagnosed were chosen: autism, type II diabetes mellitus and Parkinson’s disease dementia. We identified the first occurrence of the condition ICD-10 code for a patient during the period April 2013 to March 2021 and in subsequent hospital spells. We designed and trained random forest classifiers to identify variables strongly associated with recording inconsistencies. Results For autism, diabetes and Parkinson’s disease dementia respectively, 43.7%, 8.6% and 31.2% of subsequent spells had inconsistencies. Coding inconsistencies were highly correlated with non-coding of an underlying condition, a change in hospital trust and greater time between the spell with the first coded diagnosis and the subsequent spell. For patients with diabetes or Parkinson’s disease dementia, the code recording for spells without an overnight stay were found to have a higher rate of inconsistencies. Conclusions Data inconsistencies are relatively common for the three conditions considered. Where these mandatory diagnoses are not recorded in administrative datasets, and where clinical decisions are made based on such data, there is potential for this to impact patient care.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KTgDPmVzHVKf"
   },
   "outputs": [],
   "source": [
    "text3 = \"The COVID-19 pandemic has created unprecedented challenges for governments and healthcare systems worldwide, highlighting the critical importance of understanding the factors that contribute to virus transmission. This study aimed to identify the most influential age groups in COVID-19 infection rates at the US county level using the Modified Morris Method and deep learning for time series. Our approach involved training the state-of-the-art time-series model Temporal Fusion Transformer on different age groups as a static feature and the population vaccination status as the dynamic feature. We analyzed the impact of those age groups on COVID-19 infection rates by perturbing individual input features and ranked them based on their Morris sensitivity scores, which quantify their contribution to COVID-19 transmission rates. The findings are verified using ground truth data from the CDC and US Census, which provide the true infection rates for each age group. The results suggest that young adults were the most influential age group in COVID-19 transmission at the county level between March 1, 2020, and November 27, 2021. Using these results can inform public health policies and interventions, such as targeted vaccination strategies, to better control the spread of the virus. Our approach demonstrates the utility of feature sensitivity analysis in identifying critical factors contributing to COVID-19 transmission and can be applied in other public health domains.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YRRYafkgANRu"
   },
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "test_dataset.append(text)\n",
    "test_dataset.append(text1)\n",
    "test_dataset.append(text2)\n",
    "test_dataset.append(text3)\n",
    "test_dataset.append(text4)\n",
    "test_dataset.append(text5)\n",
    "test_dataset.append(text6)\n",
    "test_dataset.append(text7)\n",
    "test_dataset.append(text8)\n",
    "test_dataset.append(text9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_2huPY-Jjs8"
   },
   "source": [
    "clean the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rgthKpAAFHq",
    "outputId": "502fbf86-3a12-456f-f699-3b77584a71eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "x_clean_test = [re.sub(r'[^a-zA-Z\\s]', u'',sentence.replace('\\n', ''), flags=re.UNICODE) for sentence in test_dataset]\n",
    "x_clean_test = [' '.join(list(filter(lambda w: w not in stop_words and len(w)>2, map(lemmatizer.lemmatize, sentence.lower().split())))[:200]) for sentence in x_clean_test]\n",
    "x_clean_test_array = [list(filter(lambda w: w not in stop_words and len(w)>2, map(lemmatizer.lemmatize, sentence.lower().split())))[:200] for sentence in x_clean_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxz8GLO1MBrs"
   },
   "source": [
    "extract research target, method, material from testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "723f31f2-12d7-48cc-9f88-1c8fbe860f4c",
    "outputId": "e6a73392-f2d4-4e14-fbab-bf355e3b4166"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background To gain maximum insight from large administrative healthcare datasets it is important to understand their data quality.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'ma', 'ma', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'o']\n",
      "Although a gold standard against which to assess criterion validity rarely exists for such datasets, internal consistency can be evaluated.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'met', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "We aimed to identify inconsistencies in the recording of mandatory International Statistical Classification of Diseases and Related Health Problems, tenth revision (ICD- 10) codes within the Hospital Episodes Statistics dataset in England.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 'o', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'o']\n",
      "Methods Three exemplar medical conditions where recording is mandatory once diagnosed were chosen: autism, type II diabetes mellitus and Parkinson’s disease dementia.\n",
      "['ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 'o', 't', 'o']\n",
      "We identified the first occurrence of the condition ICD-10 code for a patient during the period April 2013 to March 2021 and in subsequent hospital spells.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "We designed and trained random forest classifiers to identify variables strongly associated with recording inconsistencies.\n",
      "['o', 'o', 'o', 'o', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Results For autism, diabetes and Parkinson’s disease dementia respectively, 43.7%, 8.6% and 31.2% of subsequent spells had inconsistencies.\n",
      "['o', 'o', 't', 'o', 'o', 'o', 't', 't', 't', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Coding inconsistencies were highly correlated with non-coding of an underlying condition, a change in hospital trust and greater time between the spell with the first coded diagnosis and the subsequent spell.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "For patients with diabetes or Parkinson’s disease dementia, the code recording for spells without an overnight stay were found to have a higher rate of inconsistencies.\n",
      "['o', 'o', 'o', 'o', 'o', 't', 't', 't', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Conclusions Data inconsistencies are relatively common for the three conditions considered.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Where these mandatory diagnoses are not recorded in administrative datasets, and where clinical decisions are made based on such data, there is potential for this to impact patient care.\n",
      "['o', 'o', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 'o']\n",
      "material  ['large administrative healthcare datasets', 'the Hospital Episodes Statistics', 'Hospital Episodes Statistics dataset', 'England', 'administrative datasets']\n",
      "research target  ['the recording', 'mandatory International Statistical Classification', 'Diseases', 'England', 'Related Health Problems', 'disease dementia', 'autism', 'Parkinson’s disease dementia', 'Parkinson’s disease dementia', 'these mandatory diagnoses', 'patient care']\n",
      "method ['random forest classifiers']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  flag = False\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  '''if len(matching_genric_array) > 0:\n",
    "    print(\"this sentence is related to \", matching_genric_array[0])\n",
    "  else:\n",
    "    if len(matching_material_array) > 0:\n",
    "      print(\"this sentence is related to material use\")\n",
    "      flag = True\n",
    "    if len(matching_method_array) > 0:\n",
    "      print(\"this sentence is related to methodology use\")\n",
    "      flag = True\n",
    "    if len(matching_metric_array) > 0:\n",
    "      print(\"this sentence is related to metric methodology\")\n",
    "      flag = True\n",
    "    if len(matching_target_array) > 0 and flag == False:\n",
    "      print(\"this sentence is related to the topic\")\n",
    "  print(\"\\n\")'''\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)\n",
    "sorted_items = sorted(map_sentence_to_keyword_occurance.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_dict = dict(sorted_items)\n",
    "key_sentence = []\n",
    "wanted_top_sentences_number = int(len(sorted_dict)/2)+1\n",
    "if wanted_top_sentences_number>6:\n",
    "  wanted_top_sentences_number = 6\n",
    "top_sentences = list(sorted_dict.keys())[:wanted_top_sentences_number]\n",
    "sentences_order = {}\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "  sentences_order[sentence] = i\n",
    "  i+=1\n",
    "for sentence in top_sentences:\n",
    "  sentences_order[sentence] = len(sentences_order)\n",
    "top_sentences = [key for key, value in sentences_order.items() if value == len(sentences_order)]\n",
    "#print(top_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0vmSdvpLB04",
    "outputId": "8daf6aff-7d04-4f16-b1c5-9de7e9efcd2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction Large healthcare datasets can provide insight that has the potential to improve outcomes for patients.\n",
      "['o', 'ma', 'ma', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "However, it is important to understand the strengths and limitations of such datasets so that the insights they provide are accurate and useful.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The aim of this study was to identify data inconsistencies within the Hospital Episodes Statistics (HES) dataset for autistic patients and assess potential biases introduced through these inconsistencies and their impact on patient outcomes.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'o', 'o', 'o', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The study can only identify inconsistencies in recording of autism diagnosis and not whether the inclusion or exclusion of the autism diagnosis is the error.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'o', 'o', 'ot', 'o', 'o', 'o', 'o', 'o']\n",
      "Data were extracted from the HES database for the period 1st April 2013 to 31st March 2021 for patients with a diagnosis of autism.\n",
      "['o', 'o', 'o', 'o', 'o', 'ma', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 'o']\n",
      "First spells in hospital during the study period were identified for each patient and these were linked to any subsequent spell in hospital for the same patient.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Data inconsistencies were recorded where autism was not recorded as a diagnosis in a subsequent spell.\n",
      "['o', 'o', 'o', 'o', 'o', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Features associated with data inconsistencies were identified using a random forest classifiers and regression modelling.\n",
      "['o', 'o', 'o', 'ot', 'ot', 'o', 'o', 'o', 'me', 'me', 'me', 'me', 'o', 'me', 'me', 'o']\n",
      "Results Data were available for 172,324 unique patients who had been recorded as having an autism diagnosis on first admission.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "In total, 43.7 % of subsequent spells were found to have inconsistencies.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'o']\n",
      "The features most strongly associated with inconsistencies included greater age, greater deprivation, longer time since the first spell, change in provider, shorter length of stay, being female and a change in the main specialty description.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The random forest algorithm had an area under the receiver operating characteristic curve of 0.864 in predicting a data inconsistency.\n",
      "['o', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'ot', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'o']\n",
      "For patients who died in hospital, inconsistencies in their final spell were significantly associated with being 80 years and over, being female, greater deprivation and use of a palliative care code in the death spell.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'o', 'ot', 'o', 'o', 'o', 'o', 'o']\n",
      "conclusions Data inconsistencies in the HES database were relatively common in autistic patients and were associated a number of patient and hospital admission characteristics.\n",
      "['o', 'o', 'o', 'o', 'o', 'ma', 'ma', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'ot', 'ot', 'ot', 'o']\n",
      "Such inconsistencies have the potential to distort our understanding of service use in key demographic groups.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 'o', 'o', 'o', 'o', 'o']\n",
      "material  ['Introduction Large healthcare datasets', 'the Hospital Episodes Statistics (HES', 'dataset', 'the HES database', 'the HES database']\n",
      "research target  ['autistic patients', 'recording', 'autism diagnosis', 'autism', 'autistic patients', 'service use']\n",
      "method ['a random forest classifiers', 'regression modelling', 'The random forest algorithm']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text1)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  flag = False\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gox5pH4KLo9o",
    "outputId": "209a3ab1-0d13-4ae6-f3e4-701934a34a49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contraction of the heart muscle is triggered by self-organizing electrical patterns.\n",
      "['o', 'ot', 'o', 'ot', 'ot', 'ot', 'o', 'o', 'o', 'ot', 'ot', 'ot', 'ot', 'ot', 'o']\n",
      "Abnormalities in these patterns lead to cardiac arrhythmias, a prominent cause of mortality worldwide.\n",
      "['o', 'o', 'o', 'ge', 'o', 'o', 't', 't', 'o', 'o', 'o', 't', 'o', 't', 'o', 'o']\n",
      "The targeted treatment or prevention of arrhythmias requires a thorough understanding of the interacting wavelets, vortices and conduction block sites within the excitation pattern.\n",
      "['o', 'o', 't', 'o', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'o', 'ot', 'o', 'ot', 'ot', 'ot', 'o', 'ot', 'ot', 'ot', 'o']\n",
      "Currently, there is no conceptual framework that covers the elementary processes during arrhythmogenesis in detail, in particular the transient pivoting patterns observed in patients, which can be interleaved with periods of less fragmented waves.\n",
      "['o', 'o', 'o', 'o', 'o', 'ge', 'ge', 'o', 'o', 'o', 'o', 'ge', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'ot', 'ot', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'ot', 'ot', 'ot', 'ot', 'ot', 'o']\n",
      "Here, we provide such a framework in terms of quasiparticles and Feynman diagrams, which were originally developed in theoretical physics.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'ot', 'o', 'ot', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 'o']\n",
      "We identified three different quasiparticles in excitation patterns: heads, tails and pivots.\n",
      "['o', 'o', 'o', 'o', 'ot', 'o', 'ot', 'ot', 'o', 'ot', 'o', 'ot', 'o', 'ot', 'o']\n",
      "In simulations and experiments, we show that these basic building blocks can combine into at least four different bound states.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'o', 'o']\n",
      "By representing their interactions as Feynman diagrams, the creation and annihilation of rotor pairs are shown to be sequences of dynamical creation, annihilation and recombination of the identified quasiparticles.\n",
      "['o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'o']\n",
      "Our results provide a new theoretical foundation for a more detailed theory, analysis and mechanistic insights of topological transitions in excitation patterns, to be applied within and beyond the context of cardiac electrophysiology.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 't', 'ot', 'ot', 'ot', 'ot', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 'o']\n",
      "material  []\n",
      "research target  ['cardiac arrhythmias', 'a prominent cause', 'mortality', 'The targeted treatment', 'prevention', 'arrhythmias', 'arrhythmogenesis', 'theoretical physics', 'cardiac electrophysiology']\n",
      "method ['Feynman diagrams']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text2)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQbo7zhvL4rb",
    "outputId": "9905d47d-46df-400f-c29e-72b8b293b746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COVID-19 pandemic has created unprecedented challenges for governments and healthcare systems worldwide, highlighting the critical importance of understanding the factors that contribute to virus transmission.\n",
      "['o', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 'o']\n",
      "This study aimed to identify the most influential age groups in COVID-19 infection rates at the US county level using the Modified Morris Method and deep learning for time series.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'o', 'o', 'o']\n",
      "Our approach involved training the state-of-the-art time-series model Temporal Fusion Transformer on different age groups as a static feature and the population vaccination status as the dynamic feature.\n",
      "['o', 'ge', 'o', 'o', 'o', 'ge', 'me', 'me', 'me', 'o', 'me', 'o', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'ot', 'ot', 'o', 'o', 'ot', 'ot', 'ot', 'o', 'o', 'ot', 'ot', 'o']\n",
      "We analyzed the impact of those age groups on COVID-19 infection rates by perturbing individual input features and ranked them based on their Morris sensitivity scores, which quantify their contribution to COVID-19 transmission rates.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 't', 'o', 'ot', 'o', 'ot', 'ot', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'met', 'met', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 't', 'o']\n",
      "The findings are verified using ground truth data from the CDC and US Census, which provide the true infection rates for each age group.\n",
      "['o', 'o', 'o', 'o', 'o', 'ma', 'ma', 'ma', 'o', 'o', 'ma', 'o', 'ma', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'o', 'o', 'o', 'o', 'o']\n",
      "The results suggest that young adults were the most influential age group in COVID-19 transmission at the county level between March 1, 2020, and November 27, 2021.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Using these results can inform public health policies and interventions, such as targeted vaccination strategies, to better control the spread of the virus.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Our approach demonstrates the utility of feature sensitivity analysis in identifying critical factors contributing to COVID-19 transmission and can be applied in other public health domains.\n",
      "['o', 'ge', 'o', 'o', 'o', 'o', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 'o', 'o']\n",
      "material  ['ground truth data', 'the CDC', 'US Census']\n",
      "research target  ['The COVID-19 pandemic', 'virus transmission', 'COVID-19 infection rates', 'the US county level', 'COVID-19 infection rates', 'COVID-19 transmission rates', 'COVID-19 transmission', 'the county level', 'COVID-19 transmission', 'other public health domains']\n",
      "method ['the Modified Morris Method', 'Modified Morris Method', 'deep learning', 'time series', 'time-series model Temporal Fusion Transformer', 'the-art', 'feature sensitivity analysis']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text3)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fwVmPgccMNZT",
    "outputId": "51b8fcfb-c19c-4689-b061-17b643076b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We show that the transformation of a time-evolving spherically symmetric metric tensor into a Painleve-Gullstrand-Lemaitre form brings forth a few curious consequences.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The time evolution describes a non-singular gravitational collapse, leading to a bounce and dispersal of all the clustered matter, or a wormhole geometry for certain initial conditions.\n",
      "['o', 'me', 'me', 'o', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The null convergence condition is violated only at the onset of bounce or the wormhole formation.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "As an example, the requirements to develop a Simpson-Visser wormhole/regular black-hole geometry is discussed.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The solution can be regarded as a new time-evolving twin of sonic dumb holes found in analog gravity.\n",
      "['o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "material  []\n",
      "research target  ['a non-singular gravitational collapse']\n",
      "method ['a time-evolving spherically symmetric metric tensor', 'The time', 'evolution', 'a Simpson-Visser wormhole/regular black-hole geometry', 'sonic dumb holes']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text4)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDtyq--kMzpS",
    "outputId": "12152ff8-2a8d-4a3c-8c5b-8ad7d9b20805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The advancement of neutrino observatories has sparked a surge in multi-messenger astronomy.\n",
      "['o', 'o', 'o', 't', 't', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 'o']\n",
      "Multiple neutrino associations among blazars are reported while neutrino production sites are located within their central (sub)parsecs.\n",
      "['o', 't', 't', 'o', 'o', 'o', 'o', 'o', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Yet, many questions remain on the nature of those processes.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o']\n",
      "The next generation Event Horizon Telescope (ngEHT) is uniquely positioned for these studies, as its high frequency and resolution can probe both the accretion disk region and the parsec-scale jet.\n",
      "['o', 'o', 'o', 'me', 'me', 'me', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'o', 'met', 'o', 'met', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "This opens up new opportunities for connecting the two regions and unraveling the proton acceleration and neutrino production in blazars.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 't', 't', 't', 't', 't', 'o', 'o', 'o']\n",
      "We outline observational strategies for ngEHT and highlight what it can contribute to the multi-messenger study of blazars.\n",
      "['o', 'o', 'ge', 'ge', 'o', 'me', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 't', 't', 'o']\n",
      "material  ['neutrino production sites']\n",
      "research target  ['neutrino observatories', 'multi-messenger astronomy', 'Multiple neutrino associations', 'the proton acceleration', 'neutrino production', 'the multi-messenger study', 'blazars']\n",
      "method ['Event Horizon Telescope', 'ngEHT', 'ngEHT']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text5)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwTa8xT5AbAK",
    "outputId": "31979143-8deb-4b7c-eeaf-e5c9ee7f0389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A challenging aspect of the description of a tokamak disruption is evaluating the hot tail runaway electron (RE) seed that emerges during the thermal quench.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "This problem is made challenging due to the requirement of describing a strongly non-thermal electron distribution, together with the need to incorporate a diverse range of multiphysics processes including magnetohydrodynamic instabilities, impurity transport, and radiative losses.\n",
      "['o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The present work develops a physics-informed neural network (PINN) tailored to the solution of the hot tail seed during an idealized axisymmetric thermal quench.\n",
      "['o', 'o', 'o', 'o', 'o', 'me', 'me', 'me', 'me', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'o']\n",
      "Here, a PINN is developed to identify solutions to the adjoint relativistic Fokker-Planck equation in the presence of a rapid quench of the plasma's thermal energy.\n",
      "['o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "It is shown that the PINN is able to accurately predict the hot tail seed across a range of parameters including the thermal quench time scale, initial plasma temperature, and local current density, in the absence of experimental or simulation data.\n",
      "['o', 'o', 'o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'met', 'o', 'o', 'o', 'met', 'met', 'o', 'o', 'o', 'ma', 'o', 'ma', 'ma', 'ma', 'ma', 'o']\n",
      "The hot tail PINN is verified by comparison with a direct Monte Carlo solution, with excellent agreement found across a broad range of thermal quench conditions.\n",
      "['o', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'me', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "material  ['the absence', 'experimental or simulation data']\n",
      "research target  ['a tokamak disruption', 'an idealized axisymmetric thermal quench']\n",
      "method ['a strongly non-thermal electron distribution', 'multiphysics processes', 'a physics-informed neural network', 'PINN', 'a PINN', 'the PINN', 'The hot tail PINN', 'a direct Monte Carlo solution']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text6)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DwO-kdyAeIX",
    "outputId": "1443a3b7-5dec-41a2-8771-d9119bf43c04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy measures quantify the amount of information and correlations present in a quantum system.\n",
      "['me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o']\n",
      "In practice, when the quantum state is unknown and only copies thereof are available, one must resort to the estimation of such entropy measures.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'me', 'me', 'me', 'o']\n",
      "Here we propose a variational quantum algorithm for estimating the von Neumann and Rényi entropies, as well as the measured relative entropy and measured Rényi relative entropy.\n",
      "['o', 'o', 'o', 'o', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'met', 'met', 'met', 'met', 'o', 'met', 'met', 'o']\n",
      "Our approach first parameterizes a variational formula for the measure of interest by a quantum circuit and a classical neural network, and then optimizes the resulting objective over parameter space.\n",
      "['o', 'ge', 'o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'o', 'me', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Numerical simulations of our quantum algorithm are provided, using a noiseless quantum simulator.\n",
      "['o', 'me', 'o', 'o', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'me', 'o']\n",
      "The algorithm provides accurate estimates of the various entropy measures for the examples tested, which renders it as a promising approach for usage in downstream tasks.\n",
      "['o', 'ge', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 'ge', 'o', 'o', 'o', 't', 't', 'o']\n",
      "material  []\n",
      "research target  ['downstream tasks']\n",
      "method ['Entropy measures', 'a quantum system', 'the estimation', 'such entropy measures', 'a variational quantum', 'algorithm', 'a variational formula', 'a quantum circuit', 'a classical neural network', 'Numerical simulations', 'quantum', 'algorithm', 'a noiseless quantum simulator', 'the various entropy measures']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text7)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJo0OmVQAgU4",
    "outputId": "f869064b-326b-4a1a-fb53-586ad2876140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formation history of giant planets inside and outside the solar system remains unknown.\n",
      "['o', 't', 'o', 'o', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "We suggest that runaway gas accretion is initiated only at a mass of ~100 M_Earth and that this mass corresponds to the transition to a gas giant, a planet that its composition is dominated in hydrogen and helium.\n",
      "['o', 'o', 'o', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Delaying runaway accretion to later times (a few Myr) and higher masses is likely to be a result of an intermediate stage of efficient heavy-element accretion (at a rate of ~10^-5 M_Earth/yr) that provides sufficient energy to hinder rapid gas accretion.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'o', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 't', 'o']\n",
      "This may imply that Saturn has never reached runaway gas accretion, and that it is a failed giant planet.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The transition to a gas giant planet above Saturn's mass naturally explains the differences between the bulk metallicities and internal structures of Jupiter and Saturn.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "The transition mass to a gas giant planets strongly depends on the exact formation history and birth environment of the planets, which are still not well constrained for our Solar System.\n",
      "['o', 'o', 'o', 'o', 'o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "In terms of giant exoplanets, delaying runaway gas accretion to planets beyond Saturn's mass can explain the transitions in the mass-radius relations of observed exoplanets and the high metallicity of intermediate-mass exoplanets.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "material  []\n",
      "research target  ['The formation history', 'giant planets', 'runaway gas accretion', 'a gas giant planets']\n",
      "method ['an intermediate stage', 'efficient heavy-element accretion']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text8)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "447eqTpDAitX",
    "outputId": "3e1f14ed-32e7-4243-c52c-3533f483e2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed scaling relations in galaxies between baryons and dark matter global properties are key to shed light on the process of galaxy formation and on the nature of dark matter.\n",
      "['o', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 't', 'o']\n",
      "Here, we study the scaling relation between the neutral hydrogen (HI) and dark matter mass in isolated rotationally-supported disk galaxies at low redshift.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "We first show that state-of-the-art galaxy formation simulations predict that the HI-to-dark halo mass ratio decreases with stellar mass for the most massive disk galaxies.\n",
      "['o', 'o', 'o', 'o', 'ge', 'me', 'o', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "We then infer dark matter halo masses from high-quality rotation curve data for isolated disk galaxies in the local Universe, and report on the actual universality of the HI-to-dark halo mass ratio for these observed galaxies.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'ma', 'ma', 'ma', 'ma', 'ma', 'ma', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "This scaling relation holds for disks spanning a range of 4 orders of magnitude in stellar mass and 3 orders of magnitude in surface brightness.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'met', 'o']\n",
      "Accounting for the diversity of rotation curve shapes in our observational fits decreases the scatter of the HI-to-dark halo mass ratio while keeping it constant.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "This finding extends the previously reported discrepancy for the stellar-to-halo mass relation of massive disk galaxies within galaxy formation simulations to the realm of neutral atomic gas.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 't', 't', 't', 'o', 'o', 'o', 't', 't', 'o', 'o', 'o', 'o', 't', 't', 't', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "Our result reveals that isolated galaxies with regularly rotating extended HI disks are surprisingly self-similar up to high masses, which hints at mass-independent self-regulation mechanisms that have yet to be fully understood.\n",
      "['o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'me', 'me', 'o', 'me', 'me', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
      "material  ['high-quality rotation curve data']\n",
      "research target  ['Observed scaling relations', 'galaxy formation', 'dark matter', 'galaxy formation simulations']\n",
      "method ['HI-to-dark', 'mass-independent self-regulation mechanisms']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text9)\n",
    "map_sentence_to_keyword_occurance = {}\n",
    "total_matching_material_array = []\n",
    "total_matching_target_array = []\n",
    "total_matching_method_array = []\n",
    "total_matching_metric_array = []\n",
    "for sentence in sentences:\n",
    "  matching_material_array, matching_target_array, matching_method_array, matching_genric_array,  matching_metric_array, total_length = evaluate_one_text(m, sentence)\n",
    "  total_matching_material_array += matching_material_array\n",
    "  total_matching_target_array += matching_target_array\n",
    "  total_matching_method_array += matching_method_array\n",
    "  total_matching_metric_array += matching_metric_array\n",
    "  map_sentence_to_keyword_occurance[sentence] = total_length\n",
    "print(\"material \", total_matching_material_array)\n",
    "print(\"research target \",total_matching_target_array)\n",
    "print(\"method\", total_matching_method_array)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
